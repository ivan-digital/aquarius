{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIa9HDhcll-v"
      },
      "source": [
        "# Инференс LLM (server-версия) с vLLM и балансировка нагрузки\n",
        "\n",
        "В этом ноутбуке мы рассмотрим основные аспекты инференса больших языковых моделей (LLM) с помощью сервера [vLLM](https://github.com/vllm-project/vllm) в контейнерах Docker. Также затронем вопросы масштабирования и балансировки нагрузки. Затрнонем Tensor и Pipeline Parralelism. Рабочий пример инференса языковой модели c Docker можно найти в github [LLM inference project](https://github.com/ivan-digital/aquarius/tree/main/inference).\n",
        "\n",
        "## 1. Серверный сценарий vs локальный Python\n",
        "При **локальном** запуске нужно установить `vllm`, `transformers` и т.п. обычным `pip install`, а затем вызывать `LLM.generate(...)`. Но в **продакшен**-среде обычно:\n",
        "- У вас есть **Docker-контейнер** или подобный механизм, где модель развёрнута в виде **серверного приложения**.\n",
        "- Клиенты (web-приложение, другие сервисы) отправляют запросы на API vLLM (REST/HTTP или gRPC).\n",
        "- В случае больших нагрузок мы создаём **несколько экземпляров** (реплик) сервиса и распределяем между ними запросы.\n",
        "\n",
        "## 2. Что такое инференс?\n",
        "Инференс (inference) — это процесс генерации ответа на основе обученной большой языковой модели, в нашем случае — модели `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`. С точки зрения пользователя, это выглядит так:\n",
        "1. Отправляем запрос с промптом (prompt) в сервис.\n",
        "2. Модель (в backend) обрабатывает токены, обновляет кэш внимания (attention key-value) и начинает процесс декодирования (beam search, top-k, и т. д.).\n",
        "3. Возвращается сгенерированный ответ.\n",
        "\n",
        "## 3. Техники генерации\n",
        "### 3.1 Beam Search\n",
        "Beam Search — жадно-детерминированный алгоритм декодирования, в котором на каждом шаге поддерживается заданное число (ширина beam, k) наиболее вероятных гипотез последовательности. После генерации всех возможных продолжений из этих k веток выбираются снова k лучших.\n",
        "\n",
        "Позволяет находить более вероятные целиком сформированные фрагменты текста по сравнению с жадным (greedy) поиском.\n",
        "Даёт более стабильные и «правдоподобные» ответы, но при этом снижает разнообразие в сравнении с методами сэмплинга (особенно при высокой температуре).\n",
        "\n",
        "### 3.2 Top-k / Top-p (Nucleus) Sampling\n",
        "- **Top-k**: выбираем следующий токен из k наиболее вероятных.\n",
        "- **Top-p**: выбираем токен из области, чья суммарная вероятность не превышает p.\n",
        "Обе техники делают ответы более разнообразными.\n",
        "\n",
        "### 3.3 Температура\n",
        "Параметр температуры (T) — коэффициент масштабирования логитов перед применением softmax, влияющий на «остроту» распределения вероятностей при выборе следующего токена.\n",
        "\n",
        "- При T = 1.0 преобразование логитов не меняет исходное распределение: модель выбирает токены строго пропорционально их вероятностям.\n",
        "- При T < 1.0 (например, 0.7) распределение становится более «резким»: наиболее вероятные токены получают ещё больший вес, и ответы выходят более детерминированными.\n",
        "- При T > 1.0 (например, 1.3) распределение «сглаживается»: вероятности менее частотных токенов растут, за счёт чего генерация становится более разнообразной и «креативной».\n",
        "\n"
      ],
      "id": "KIa9HDhcll-v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Оптимизации для LLM-инференса\n",
        "### 4.1 Paged Attention\n",
        "Paged Attention — это метод управления памятью при вычислении механизма внимания (attention) в трансформерах. Основная проблема, которую он решает, — это резкое возрастание потребления GPU-памяти при увеличении длины последовательности токенов. В стандартном механизме внимания память расходуется пропорционально O(n<sup>2</sup>), где n — длина входной последовательности. Для больших n объём выделенной памяти может быстро выйти за пределы доступного пула на видеокарте.\n",
        "\n",
        "Paged Attention разбивает вычисления на «страницы» (pages) или блоки — подобно тому, как работает виртуальная память на уровне операционной системы.\n",
        "\n",
        "### 4.2 FlashAttention / Kernel Fusion\n",
        "FlashAttention — это высокоэффективная реализация механизма внимания, которая минимизирует число обращений к памяти и перестраивает логику вычислений таким образом, чтобы аккумулировать промежуточные результаты прямо в регистрах GPU и/или блоках быстрой кэш-памяти. При обычном подходе каждый элемент внимания требует многократных операций чтения/записи с/в глобальную память GPU, что довольно «дорого» по времени.\n",
        "\n",
        "### 4.3 Квантование (int8, int4)\n",
        "Квантование переводит веса модели из более высокой точности (FP32, FP16, BF16) в более низкую (например, int8, int4). Цель — уменьшить:\n",
        "\n",
        "Объём памяти, занимаемый моделью (порой на 50–75% и более).\n",
        "Пропускную способность, требуемую для чтения/записи весов.\n",
        "Время вычислений (за счёт эффективных инструкций, например, INT8-множения на современных GPU/TPU).\n",
        "При этом может слегка (или значительно, в зависимости от агрессивности квантования) ухудшиться точность модели.\n",
        "\n",
        "#### Виды квантования\n",
        "\n",
        "- Post-training quantization (PTQ): квантование после обучения, без дополнительного тренинга. Самый простой способ, но может сильнее повлиять на точность, так как веса меняются «резко» с FP16/FP32 до int8 и т.п.\n",
        "\n",
        "- Quantization-aware training (QAT): модель обучается с учётом квантования, то есть «эмулирует» низкую точность во время обучения. Это обычно даёт более высокое качество результирующей модели, но требует дополнительного времени и ресурсов на тренировку.\n",
        "\n",
        "- Mixed Precision: можно квантовать не все части модели, а только, например, некоторые слои или определённые типы операций (активации, веса и т. д.). Это помогает найти баланс между производительностью и качеством.\n",
        "\n",
        "### 4.4 Speculative decoding\n",
        "Генерация выполняется сразу двумя моделями: маленькая draft-model предлагает пачку токенов, а большая target-model проверяет и «отбрасывает» неподходящие, тем самым экономя вызовы тяжёлой модели и ускоряя вывод до ×2–×4 на тех же GPU. [vllm документация](https://docs.vllm.ai/en/latest/features/spec_decode.html) Задействуется через специальный параметр `speculative_config`.\n"
      ],
      "metadata": {
        "id": "vwzGykzpiV1T"
      },
      "id": "vwzGykzpiV1T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Балансировка нагрузки (Load Balancing)\n",
        "При **увеличении числа запросов** вы можете масштабировать сервис, поднимая **несколько** инстансов vLLM. Каждый контейнер будет использовать тот же образ (с той же моделью `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`) — всё зависит от вашей GPU-инфраструктуры:\n",
        "- Если у вас несколько GPU на одном сервере, можно поднять столько контейнеров, сколько GPU.\n",
        "- Если есть несколько серверов, каждый с GPU, можно организовать кластер (docker swarm, kubernetes и т. д.).\n",
        "\n",
        "### 5.1 Роль Nginx\n",
        "Nginx (или другой веб-сервер/прокси) может выступать в роли **reverse proxy** и **load balancer**:\n",
        "1. **Принимает** входящие запросы клиентов (запросы к REST API).\n",
        "2. **Распределяет** эти запросы по нескольким контейнерам vLLM. Например, используя `upstream` в `nginx.conf`.\n",
        "3. Возвращает клиенту ответ от одного из экземпляров.\n",
        "\n",
        "Таким образом, когда нагрузка растёт, вы поднимаете больше реплик (например, `replicas: 2` или `replicas: 3` и т. д.), и Nginx \"размазывает\" входящий трафик по ним."
      ],
      "metadata": {
        "id": "UDCCBMm1igG6"
      },
      "id": "UDCCBMm1igG6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Параллелизм модели (Рассмотрим теоретически)\n",
        "Более детальный материал [Model Parralelism](https://huggingface.co/docs/transformers/v4.13.0/parallelism) на сайте transformers. И туториал от vllm о том, [как выбирать распределенную архитектуру инференса языковых моделей](https://docs.vllm.ai/en/v0.5.2/serving/distributed_serving.html).\n",
        "\n",
        "### 6.1 Tensor Parallelism (TP)\n",
        "\n",
        "Огромные весовые матрицы делятся по строкам/столбцам; каждая GPU хранит и умножает лишь свою «долю» тензора .\n",
        "vLLM-флаг. `--tensor-parallel-size <N>` — число GPU в узле, участвующих в TP.\n",
        "\n",
        "Применяется эта техника когда модель не помещается в память одной GPU, но суммарная память узла достаточна; при наличии NVLink задержка минимальна.\n",
        "\n",
        "### 6.2 Pipeline Parallelism (PP)\n",
        "\n",
        "Полные слои трансформера распределяются по разным процессам/узлам; микробатчи проходят через «конвейер» из K стадий.\n",
        "vLLM-флаг. `--pipeline-parallel-size <K>` — число стадий; на мульти-ноде vLLM автоматически переключается c multiprocessing на Ray для оркестрации.\n",
        "\n",
        "Комбинация с TP. Итоговое число GPU = tp × pp; например, tp = 8 (8 GPU на узле) и pp = 2 (2 узла) задействуют 16 GPU и позволяют поместить 70-миллиардную модель, не увеличивая задержку в 8 раз\n",
        "vLLM.\n",
        "\n",
        "### 6.3 Сетевые настройки и NCCL\n",
        "TP/PP обмениваются тензорами через NCCL, неверный выбор интерфейса приводит к тайм-аутам и «просадкам» пропускной способности.\n",
        "\n",
        "- `NCCL_SOCKET_IFNAME=eth0` — явно выбирает NIC, исключая docker0 и прочие виртуальные интерфейсы;\n",
        "- `NCCL_P2P_DISABLE=1` — отключает peer-to-peer оптимизацию, если узел без NVLink и работает только через PCIe, что ускоряет инициализацию TP;\n",
        "- `NCCL_DEBUG=warn` — выводит предупреждения-«зависания» барьеров, полезно при первых тестах multi-node."
      ],
      "metadata": {
        "id": "pH5Vi7Y5rpDJ"
      },
      "id": "pH5Vi7Y5rpDJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Пример структуры Docker Compose\n",
        "Тут — та же концепция, что и в изначальном примере, но здесь можно поменять `replicas: 1` на большее число (например, `replicas: 2`) в секции `deploy` для `vllm-api`. Nginx будет маршрутизировать запросы.\n",
        "\n",
        "```yaml\n",
        "services:\n",
        "  vllm-api:\n",
        "    build: ./vllm\n",
        "    container_name: vllm-api\n",
        "    environment:\n",
        "      - NVIDIA_VISIBLE_DEVICES=all\n",
        "    deploy:\n",
        "      replicas: 1  # Можно увеличить, если нужно масштабировать, тогда будут создаваться с разными именами\n",
        "      resources:\n",
        "        reservations:\n",
        "          devices:\n",
        "            - capabilities: [gpu]\n",
        "    ports:\n",
        "      - \"8000\"\n",
        "      - \"9464\"\n",
        "    networks:\n",
        "      - llm-net\n",
        "\n",
        "  prometheus:\n",
        "    image: prom/prometheus:v2.52.0\n",
        "    container_name: prometheus\n",
        "    volumes:\n",
        "      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n",
        "    ports:\n",
        "      - \"9090:9090\"\n",
        "    networks:\n",
        "      - llm-net\n",
        "\n",
        "  grafana:\n",
        "    image: grafana/grafana-oss:11.0.0\n",
        "    container_name: grafana\n",
        "    environment:\n",
        "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
        "    volumes:\n",
        "      - ./grafana/provisioning:/etc/grafana/provisioning:ro\n",
        "    ports:\n",
        "      - \"3000:3000\"\n",
        "    depends_on:\n",
        "      - prometheus\n",
        "    networks:\n",
        "      - llm-net\n",
        "\n",
        "  nginx:\n",
        "    image: nginx:alpine\n",
        "    container_name: nginx\n",
        "    volumes:\n",
        "      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n",
        "    ports:\n",
        "      - \"80:80\"\n",
        "    depends_on:\n",
        "      - vllm-api\n",
        "    networks:\n",
        "      - llm-net\n",
        "\n",
        "networks:\n",
        "  llm-net:\n",
        "    driver: bridge\n",
        "```\n",
        "\n",
        "Пример `nginx.conf` может содержать:\n",
        "\n",
        "```nginx\n",
        "events {\n",
        "  worker_connections  1024;\n",
        "}\n",
        "\n",
        "http {\n",
        "  upstream vllm_cluster {\n",
        "    # Несколько экземпляров (реплик) vLLM.\n",
        "    server vllm-api:8000;\n",
        "    # Если replicas > 1, дополнительно:\n",
        "    # server vllm-api-2:8000;\n",
        "    # server vllm-api-3:8000;\n",
        "  }\n",
        "\n",
        "  server {\n",
        "    listen 80;\n",
        "    location / {\n",
        "      proxy_pass http://vllm_cluster;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "b9EsxXRiiZ2U"
      },
      "id": "b9EsxXRiiZ2U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MOOKJGEll-x"
      },
      "source": [
        "## 8. Пример Dockerfile под vLLM (server)\n",
        "Ниже — Dockerfile, где мы устанавливаем все необходимые зависимости, клонируем `flashinfer`, `vllm` и настраиваем модель `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`.\n",
        "\n",
        "```dockerfile\n",
        "FROM nvcr.io/nvidia/pytorch:25.03-py3\n",
        "\n",
        "ENV MAX_JOBS=16 \\\n",
        "    NVCC_THREADS=4 \\\n",
        "    FLASHINFER_ENABLE_AOT=0 \\\n",
        "    USE_CUDA=1 \\\n",
        "    CUDA_HOME=/usr/local/cuda \\\n",
        "    TORCH_CUDA_ARCH_LIST='12.0+PTX' \\\n",
        "    CCACHE_DIR=/root/.ccache \\\n",
        "    OTEL_SERVICE_NAME=vllm-api \\\n",
        "    OTEL_METRICS_EXPORTER=prometheus \\\n",
        "    OTEL_TRACES_EXPORTER=none \\\n",
        "    OTEL_EXPORTER_PROMETHEUS_HOST=0.0.0.0 \\\n",
        "    OTEL_EXPORTER_PROMETHEUS_PORT=9464\n",
        "\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "        kmod git cmake ccache python3-pip python3-dev \\\n",
        "    && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "RUN pip3 install --upgrade pip\n",
        "RUN pip3 install bitsandbytes \\\n",
        "    opentelemetry-distro \\\n",
        "    opentelemetry-exporter-prometheus \\\n",
        "    opentelemetry-instrumentation-fastapi \\\n",
        "    opentelemetry-instrumentation-asgi\n",
        "\n",
        "RUN git clone --recursive https://github.com/flashinfer-ai/flashinfer.git /workspace/flashinfer\n",
        "WORKDIR /workspace/flashinfer\n",
        "RUN pip3 install -e . -v\n",
        "\n",
        "RUN git clone https://github.com/vllm-project/vllm.git /workspace/vllm\n",
        "WORKDIR /workspace/vllm\n",
        "RUN python3 use_existing_torch.py\n",
        "RUN pip3 install --no-cache-dir -r requirements/build.txt\n",
        "RUN pip3 install --no-cache-dir setuptools_scm\n",
        "RUN python3 setup.py develop\n",
        "\n",
        "RUN pip3 install --no-cache-dir transformers accelerate huggingface_hub\n",
        "\n",
        "# Скрипт запуска.\n",
        "COPY scripts/run.sh /workspace/run.sh\n",
        "EXPOSE 8000 9464\n",
        "CMD [ \"/workspace/run.sh\" ]\n",
        "```\n"
      ],
      "id": "5MOOKJGEll-x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCLbxpcsll-y"
      },
      "source": [
        "## 9. Скрипт запуска (run.sh)\n",
        "```bash\n",
        "#!/usr/bin/env bash\n",
        "set -e\n",
        "\n",
        "# Используем OpenTelemetry для экспорта метрик в Prometheus.\n",
        "exec opentelemetry-instrument \\\n",
        "  --metrics_exporter prometheus \\\n",
        "  --service_name \"${OTEL_SERVICE_NAME:-vllm-api}\" \\\n",
        "  vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
        "        --host 0.0.0.0 \\\n",
        "        --port 8000 \\\n",
        "        --trust-remote-code \\\n",
        "        --dtype bfloat16 \\\n",
        "        --quantization bitsandbytes\n",
        "```\n",
        "\n",
        "Обратите внимание, что здесь мы используем **server**-версию `vllm serve`, а не локальный Python-код. Модель `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` подгружается из репозитория Hugging Face.\n"
      ],
      "id": "GCLbxpcsll-y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX-VFRDAll-y"
      },
      "source": [
        "## 10. Мониторинг с OpenTelemetry и Prometheus\n",
        "- **OpenTelemetry** собирает метрики по работе сервиса (время отклика, количество запросов, использование памяти и т. д.).\n",
        "- **Prometheus** «scrape»-ит эти метрики (порт 9464) и сохраняет их.\n",
        "- **Grafana** отображает метрики в удобном виде (графики, дашборды).\n"
      ],
      "id": "PX-VFRDAll-y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aPyoXOVll-y"
      },
      "source": [
        "## 11. Пример (демонстрационный) инференса напрямую (локально)\n",
        "Хотя мы используем **server-версию**, иногда полезно показать небольшой пример **локального** вызова (не в проде!). Просто для демонстрации принципа.\n",
        "\n",
        "```python\n",
        "# Если бы мы запускали локально:\n",
        "# !pip install vllm\n",
        "from vllm import LLM, SamplingParams\n",
        "llm = LLM(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "prompt = \"Привет! Расскажи интересный факт о космосе.\"\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    max_tokens=64\n",
        ")\n",
        "response = llm.generate([prompt], sampling_params)\n",
        "print(response[0].outputs[0].text)\n",
        "```\n",
        "\n",
        "Однако в вашем случае всё завернуто в контейнер и API, поэтому прямого `llm.generate` нет — вы дергаете REST или gRPC.\n"
      ],
      "id": "6aPyoXOVll-y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JuD_1tSll-0"
      },
      "source": [
        "## 12. Итог\n",
        "1. **Запуск**: `docker-compose up -d --build`.\n",
        "2. **Сервис vLLM** поднимается на порту 8000 (для метрик 9464).\n",
        "3. **Nginx** слушает 80-й порт и маршрутизирует запросы к `vllm-api`.\n",
        "4. **Prometheus** (порт 9090) собирает метрики.\n",
        "5. **Grafana** (порт 3000) показывает дашборды.\n",
        "6. **Масштабирование**: если нужно обслуживать больше клиентов — увеличивайте `replicas` у `vllm-api` и добавляйте записи `server` в `nginx.conf` (или используйте динамический сервис-дискавери, если это Kubernetes). Так же мы рассмотрели **Tensor Parralelism** и **Pipeline Parralelism**.\n",
        "\n",
        "Таким образом, мы показали базовую архитектуру **server-версии** vLLM, пояснили, как **распределять нагрузку** между несколькими инстансами, а также коротко описали основные механизмы оптимизаций для LLM-инференса.\n"
      ],
      "id": "2JuD_1tSll-0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}